{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5chYz0k1AgKY"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "4CnPedV_WSAr",
    "outputId": "9abf25ec-8481-4642-efb4-3dee8070792b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive/\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kEw4T406WyfD"
   },
   "outputs": [],
   "source": [
    "def load_data(data_path):\n",
    "\n",
    "  with open(data_path) as training_file:\n",
    "    csv_reader=csv.reader(training_file,delimiter=',')\n",
    "    first_line=True\n",
    "    temp_images=[]\n",
    "    temp_labels=[]\n",
    "\n",
    "    for row in csv_reader:\n",
    "      if first_line:\n",
    "        first_line=False\n",
    "      else:\n",
    "        temp_labels.append(row[0])\n",
    "        image_data=row[1:]\n",
    "        image_data_as_array=np.array_split(image_data,28)\n",
    "        temp_images.append(image_data_as_array)\n",
    "    images=np.array(temp_images).astype('float')\n",
    "    labels=np.array(temp_labels).astype('float')\n",
    "\n",
    "    return images,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c-pDMij2A41l"
   },
   "outputs": [],
   "source": [
    "def load_az_dataset(datasetPath):\n",
    "  data=[]\n",
    "  labels=[]\n",
    "\n",
    "  for row in open(datasetPath):\n",
    "    row=row.split(\",\")\n",
    "    label=int(row[0])\n",
    "    image=np.array([int(x) for x in row[1:]])\n",
    "    image=image.reshape((28,28))\n",
    "\n",
    "    data=np.array(data.append(image),dtype=\"float32\")\n",
    "    labels=np.array(labels.append(labels))\n",
    "\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uPNTT7VM5Y4O"
   },
   "outputs": [],
   "source": [
    "def load_mnist_dataset():\n",
    "  ((trainData,trainLabels),(testData,testLabels))=mnist.load_data()\n",
    "  data=np.vstack([trainData,testData])\n",
    "  labels=np.hstack([trainLabels,testLabels])\n",
    "\n",
    "  return (data,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XNAh2IC0FM-b"
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tqdarZc9GPr9"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from imutils import build_montages\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-q2GsgdWg8hF"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "5iJ5yGgbPvz7",
    "outputId": "a51aad25-8bc1-42ac-e266-01cbe936b603"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets.....\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "Epochs=50\n",
    "init_lr=1e-1\n",
    "batch_size=128\n",
    "\n",
    "print(\"Loading datasets.....\")\n",
    "\n",
    "(azData, azLabels)=load_data('/content/drive/My Drive/Colab Notebooks/Py_image_search_OCR/A_Z Handwritten Data.csv')\n",
    "(digitsData,digitsLabels)=load_mnist_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "mibDTCYTbB8C",
    "outputId": "b5e4db27-a7ce-4701-e0ee-904c5335671d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(372450, 28, 28)"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "azData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ROfwLhngdTRe"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('/content/drive/My Drive/Colab Notebooks/Py_image_search_OCR/A_Z Handwritten Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    },
    "colab_type": "code",
    "id": "JaFKPl6zeyEz",
    "outputId": "a981a41b-1b1e-4e63-8fd1-dd3273b21ab5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0.1</th>\n",
       "      <th>0.2</th>\n",
       "      <th>0.3</th>\n",
       "      <th>0.4</th>\n",
       "      <th>0.5</th>\n",
       "      <th>0.6</th>\n",
       "      <th>0.7</th>\n",
       "      <th>0.8</th>\n",
       "      <th>0.9</th>\n",
       "      <th>0.10</th>\n",
       "      <th>0.11</th>\n",
       "      <th>0.12</th>\n",
       "      <th>0.13</th>\n",
       "      <th>0.14</th>\n",
       "      <th>0.15</th>\n",
       "      <th>0.16</th>\n",
       "      <th>0.17</th>\n",
       "      <th>0.18</th>\n",
       "      <th>0.19</th>\n",
       "      <th>0.20</th>\n",
       "      <th>0.21</th>\n",
       "      <th>0.22</th>\n",
       "      <th>0.23</th>\n",
       "      <th>0.24</th>\n",
       "      <th>0.25</th>\n",
       "      <th>0.26</th>\n",
       "      <th>0.27</th>\n",
       "      <th>0.28</th>\n",
       "      <th>0.29</th>\n",
       "      <th>0.30</th>\n",
       "      <th>0.31</th>\n",
       "      <th>0.32</th>\n",
       "      <th>0.33</th>\n",
       "      <th>0.34</th>\n",
       "      <th>0.35</th>\n",
       "      <th>0.36</th>\n",
       "      <th>0.37</th>\n",
       "      <th>0.38</th>\n",
       "      <th>0.39</th>\n",
       "      <th>...</th>\n",
       "      <th>0.609</th>\n",
       "      <th>0.610</th>\n",
       "      <th>0.611</th>\n",
       "      <th>0.612</th>\n",
       "      <th>0.613</th>\n",
       "      <th>0.614</th>\n",
       "      <th>0.615</th>\n",
       "      <th>0.616</th>\n",
       "      <th>0.617</th>\n",
       "      <th>0.618</th>\n",
       "      <th>0.619</th>\n",
       "      <th>0.620</th>\n",
       "      <th>0.621</th>\n",
       "      <th>0.622</th>\n",
       "      <th>0.623</th>\n",
       "      <th>0.624</th>\n",
       "      <th>0.625</th>\n",
       "      <th>0.626</th>\n",
       "      <th>0.627</th>\n",
       "      <th>0.628</th>\n",
       "      <th>0.629</th>\n",
       "      <th>0.630</th>\n",
       "      <th>0.631</th>\n",
       "      <th>0.632</th>\n",
       "      <th>0.633</th>\n",
       "      <th>0.634</th>\n",
       "      <th>0.635</th>\n",
       "      <th>0.636</th>\n",
       "      <th>0.637</th>\n",
       "      <th>0.638</th>\n",
       "      <th>0.639</th>\n",
       "      <th>0.640</th>\n",
       "      <th>0.641</th>\n",
       "      <th>0.642</th>\n",
       "      <th>0.643</th>\n",
       "      <th>0.644</th>\n",
       "      <th>0.645</th>\n",
       "      <th>0.646</th>\n",
       "      <th>0.647</th>\n",
       "      <th>0.648</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  0.1  0.2  0.3  0.4  0.5  ...  0.643  0.644  0.645  0.646  0.647  0.648\n",
       "0  0    0    0    0    0    0  ...      0      0      0      0      0      0\n",
       "1  0    0    0    0    0    0  ...      0      0      0      0      0      0\n",
       "2  0    0    0    0    0    0  ...      0      0      0      0      0      0\n",
       "3  0    0    0    0    0    0  ...      0      0      0      0      0      0\n",
       "4  0    0    0    0    0    0  ...      0      0      0      0      0      0\n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZWW0h0CjfLNZ"
   },
   "outputs": [],
   "source": [
    "azLabels+=10\n",
    "\n",
    "data=np.vstack([azData,digitsData])\n",
    "labels=np.hstack([azLabels,digitsLabels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2aV1rIUFgQ8o"
   },
   "outputs": [],
   "source": [
    "data=[cv2.resize(image,(32,32)) for image in data]\n",
    "data=np.array(data,dtype='float32')\n",
    "\n",
    "data=np.expand_dims(data,axis=-1)\n",
    "data/=255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MuS6MgKogusS"
   },
   "outputs": [],
   "source": [
    "le=LabelBinarizer()\n",
    "labels=le.fit_transform(labels)\n",
    "counts=labels.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cSqYSVnZhoqz"
   },
   "outputs": [],
   "source": [
    "classTotals=labels.sum(axis=0)\n",
    "classWeight={}\n",
    "\n",
    "for i in range(0, len(classTotals)):\n",
    "  classWeight[i]=classTotals.max()/classTotals[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8o-Jf7kVhqaS"
   },
   "outputs": [],
   "source": [
    "(trainX, testX, trainY, testY)=train_test_split(data,\n",
    "              labels,test_size=0.2,stratify=labels,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pHxQi5QwjD3I"
   },
   "outputs": [],
   "source": [
    "Datagen=ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "\t  zoom_range=0.05,\n",
    "\t  width_shift_range=0.1,\n",
    "\t  height_shift_range=0.1,\n",
    "\t  shear_range=0.15,\n",
    "\t  horizontal_flip=False,\n",
    "\t  fill_mode=\"nearest\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "XOt0k6a5sl7s",
    "outputId": "95ec2a9a-7e74-40b5-ae1b-2fc6f9af0757"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94773248/94765736 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "opt=SGD(lr=1e-1, decay=(1e-1)/50)\n",
    "\n",
    "model=ResNet50(weights='imagenet',include_top=False,input_shape=(32,32,3))\n",
    "\n",
    "x=model.output\n",
    "x=Flatten()(x)\n",
    "x=Dense(256,activation='relu')(x)\n",
    "x=Dropout(0.3)(x)\n",
    "x=Dense(36,activation='softmax')(x)\n",
    "modelFinal=Model(inputs=model.input,outputs=x)\n",
    "for layer in model.layers:\n",
    " layer.trainable = False\n",
    "\n",
    "modelFinal.compile(loss=\"categorical_crossentropy\",optimizer=opt,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IuTHJdMYlHzO"
   },
   "outputs": [],
   "source": [
    "model_checkpoint = ModelCheckpoint('Trained_Model.hdf5',\n",
    "                                    monitor='val_accuracy',\n",
    "                                    verbose=1,\n",
    "                                    mode='max',\n",
    "                                    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "04ouD_o0gDUw"
   },
   "outputs": [],
   "source": [
    "def large_model():\n",
    "  model=Sequential()\n",
    "  model.add(Conv2D(30,(5,5),input_shape=(32,32,1),activation='relu'))\n",
    "  model.add(MaxPooling2D())\n",
    "  model.add(Conv2D(15,(3,3),activation='relu'))\n",
    "  model.add(MaxPooling2D())\n",
    "  model.add(Dropout(0.2))\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(128,activation='relu'))\n",
    "  model.add(Dense(50,activation='relu'))\n",
    "  model.add(Dense(36,activation='softmax'))\n",
    "  model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 180
    },
    "colab_type": "code",
    "id": "tr-YYx7xgKb0",
    "outputId": "6e331bef-2597-4350-ed45-5d21363616c7"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-34a0aa955100>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m large_model.compile(loss=\"categorical_crossentropy\",optimizer=opt,\n\u001b[0m\u001b[1;32m      2\u001b[0m               metrics=['accuracy'])\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'compile'"
     ]
    }
   ],
   "source": [
    "large_model.compile(loss=\"categorical_crossentropy\",optimizer=opt,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "3WicOCfRvd7p",
    "outputId": "63b7899b-9d23-4b91-d1e7-481ab4dfc2cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2763/2765 [============================>.] - ETA: 0s - loss: 1.8145 - accuracy: 0.8639\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.94290, saving model to Trained_Model.hdf5\n",
      "2765/2765 [==============================] - 13s 5ms/step - loss: 1.8139 - accuracy: 0.8639 - val_loss: 0.1790 - val_accuracy: 0.9429\n",
      "Epoch 2/50\n",
      "2763/2765 [============================>.] - ETA: 0s - loss: 0.7307 - accuracy: 0.9340\n",
      "Epoch 00002: val_accuracy improved from 0.94290 to 0.95621, saving model to Trained_Model.hdf5\n",
      "2765/2765 [==============================] - 12s 5ms/step - loss: 0.7304 - accuracy: 0.9341 - val_loss: 0.1421 - val_accuracy: 0.9562\n",
      "Epoch 3/50\n",
      "2759/2765 [============================>.] - ETA: 0s - loss: 0.5914 - accuracy: 0.9449\n",
      "Epoch 00003: val_accuracy did not improve from 0.95621\n",
      "2765/2765 [==============================] - 12s 5ms/step - loss: 0.5913 - accuracy: 0.9449 - val_loss: 0.1272 - val_accuracy: 0.9539\n",
      "Epoch 4/50\n",
      "2760/2765 [============================>.] - ETA: 0s - loss: 0.5148 - accuracy: 0.9512\n",
      "Epoch 00004: val_accuracy improved from 0.95621 to 0.95978, saving model to Trained_Model.hdf5\n",
      "2765/2765 [==============================] - 13s 5ms/step - loss: 0.5149 - accuracy: 0.9512 - val_loss: 0.1246 - val_accuracy: 0.9598\n",
      "Epoch 5/50\n",
      "2765/2765 [==============================] - ETA: 0s - loss: 0.4558 - accuracy: 0.9554\n",
      "Epoch 00005: val_accuracy improved from 0.95978 to 0.96550, saving model to Trained_Model.hdf5\n",
      "2765/2765 [==============================] - 13s 5ms/step - loss: 0.4558 - accuracy: 0.9554 - val_loss: 0.1090 - val_accuracy: 0.9655\n",
      "Epoch 6/50\n",
      "2765/2765 [==============================] - ETA: 0s - loss: 0.4186 - accuracy: 0.9578\n",
      "Epoch 00006: val_accuracy improved from 0.96550 to 0.97035, saving model to Trained_Model.hdf5\n",
      "2765/2765 [==============================] - 13s 5ms/step - loss: 0.4186 - accuracy: 0.9578 - val_loss: 0.0974 - val_accuracy: 0.9703\n",
      "Epoch 7/50\n",
      "2755/2765 [============================>.] - ETA: 0s - loss: 0.3906 - accuracy: 0.9598\n",
      "Epoch 00007: val_accuracy improved from 0.97035 to 0.97341, saving model to Trained_Model.hdf5\n",
      "2765/2765 [==============================] - 13s 5ms/step - loss: 0.3909 - accuracy: 0.9598 - val_loss: 0.0886 - val_accuracy: 0.9734\n",
      "Epoch 8/50\n",
      "2762/2765 [============================>.] - ETA: 0s - loss: 0.3594 - accuracy: 0.9625\n",
      "Epoch 00008: val_accuracy did not improve from 0.97341\n",
      "2765/2765 [==============================] - 12s 5ms/step - loss: 0.3595 - accuracy: 0.9625 - val_loss: 0.1193 - val_accuracy: 0.9604\n",
      "Epoch 9/50\n",
      "2761/2765 [============================>.] - ETA: 0s - loss: 0.3402 - accuracy: 0.9647\n",
      "Epoch 00009: val_accuracy did not improve from 0.97341\n",
      "2765/2765 [==============================] - 13s 5ms/step - loss: 0.3401 - accuracy: 0.9647 - val_loss: 0.0907 - val_accuracy: 0.9714\n",
      "Epoch 10/50\n",
      "2765/2765 [==============================] - ETA: 0s - loss: 0.3239 - accuracy: 0.9657\n",
      "Epoch 00010: val_accuracy improved from 0.97341 to 0.97704, saving model to Trained_Model.hdf5\n",
      "2765/2765 [==============================] - 13s 5ms/step - loss: 0.3239 - accuracy: 0.9657 - val_loss: 0.0767 - val_accuracy: 0.9770\n",
      "Epoch 11/50\n",
      "2764/2765 [============================>.] - ETA: 0s - loss: 0.3086 - accuracy: 0.9679\n",
      "Epoch 00011: val_accuracy did not improve from 0.97704\n",
      "2765/2765 [==============================] - 13s 5ms/step - loss: 0.3086 - accuracy: 0.9679 - val_loss: 0.0816 - val_accuracy: 0.9736\n",
      "Epoch 12/50\n",
      "2759/2765 [============================>.] - ETA: 0s - loss: 0.2970 - accuracy: 0.9685\n",
      "Epoch 00012: val_accuracy did not improve from 0.97704\n",
      "2765/2765 [==============================] - 12s 5ms/step - loss: 0.2972 - accuracy: 0.9685 - val_loss: 0.0899 - val_accuracy: 0.9686\n",
      "Epoch 13/50\n",
      "2765/2765 [==============================] - ETA: 0s - loss: 0.2852 - accuracy: 0.9698\n",
      "Epoch 00013: val_accuracy did not improve from 0.97704\n",
      "2765/2765 [==============================] - 13s 5ms/step - loss: 0.2852 - accuracy: 0.9698 - val_loss: 0.0765 - val_accuracy: 0.9764\n",
      "Epoch 14/50\n",
      "2758/2765 [============================>.] - ETA: 0s - loss: 0.2735 - accuracy: 0.9704\n",
      "Epoch 00014: val_accuracy did not improve from 0.97704\n",
      "2765/2765 [==============================] - 12s 5ms/step - loss: 0.2737 - accuracy: 0.9704 - val_loss: 0.0959 - val_accuracy: 0.9653\n",
      "Epoch 15/50\n",
      "2759/2765 [============================>.] - ETA: 0s - loss: 0.2700 - accuracy: 0.9714\n",
      "Epoch 00015: val_accuracy improved from 0.97704 to 0.97999, saving model to Trained_Model.hdf5\n",
      "2765/2765 [==============================] - 13s 5ms/step - loss: 0.2699 - accuracy: 0.9714 - val_loss: 0.0660 - val_accuracy: 0.9800\n",
      "Epoch 16/50\n",
      "2755/2765 [============================>.] - ETA: 0s - loss: 0.2616 - accuracy: 0.9716\n",
      "Epoch 00016: val_accuracy improved from 0.97999 to 0.98114, saving model to Trained_Model.hdf5\n",
      "2765/2765 [==============================] - 13s 5ms/step - loss: 0.2615 - accuracy: 0.9716 - val_loss: 0.0642 - val_accuracy: 0.9811\n",
      "Epoch 17/50\n",
      "2762/2765 [============================>.] - ETA: 0s - loss: 0.2558 - accuracy: 0.9721\n",
      "Epoch 00017: val_accuracy did not improve from 0.98114\n",
      "2765/2765 [==============================] - 13s 5ms/step - loss: 0.2557 - accuracy: 0.9721 - val_loss: 0.0802 - val_accuracy: 0.9742\n",
      "Epoch 18/50\n",
      "2753/2765 [============================>.] - ETA: 0s - loss: 0.2507 - accuracy: 0.9728\n",
      "Epoch 00018: val_accuracy did not improve from 0.98114\n",
      "2765/2765 [==============================] - 13s 5ms/step - loss: 0.2507 - accuracy: 0.9728 - val_loss: 0.0889 - val_accuracy: 0.9699\n",
      "Epoch 19/50\n",
      "2753/2765 [============================>.] - ETA: 0s - loss: 0.2375 - accuracy: 0.9737\n",
      "Epoch 00019: val_accuracy did not improve from 0.98114\n",
      "2765/2765 [==============================] - 12s 5ms/step - loss: 0.2374 - accuracy: 0.9737 - val_loss: 0.0648 - val_accuracy: 0.9811\n",
      "Epoch 20/50\n",
      "2759/2765 [============================>.] - ETA: 0s - loss: 0.2372 - accuracy: 0.9735\n",
      "Epoch 00020: val_accuracy did not improve from 0.98114\n",
      "2765/2765 [==============================] - 12s 5ms/step - loss: 0.2370 - accuracy: 0.9736 - val_loss: 0.0711 - val_accuracy: 0.9785\n",
      "Epoch 21/50\n",
      "2762/2765 [============================>.] - ETA: 0s - loss: 0.2258 - accuracy: 0.9748\n",
      "Epoch 00021: val_accuracy improved from 0.98114 to 0.98178, saving model to Trained_Model.hdf5\n",
      "2765/2765 [==============================] - 13s 5ms/step - loss: 0.2258 - accuracy: 0.9749 - val_loss: 0.0657 - val_accuracy: 0.9818\n",
      "Epoch 22/50\n",
      "2764/2765 [============================>.] - ETA: 0s - loss: 0.2294 - accuracy: 0.9744\n",
      "Epoch 00022: val_accuracy did not improve from 0.98178\n",
      "2765/2765 [==============================] - 13s 5ms/step - loss: 0.2294 - accuracy: 0.9744 - val_loss: 0.0676 - val_accuracy: 0.9806\n",
      "Epoch 23/50\n",
      "2763/2765 [============================>.] - ETA: 0s - loss: 0.2281 - accuracy: 0.9747\n",
      "Epoch 00023: val_accuracy did not improve from 0.98178\n",
      "2765/2765 [==============================] - 13s 5ms/step - loss: 0.2281 - accuracy: 0.9747 - val_loss: 0.0652 - val_accuracy: 0.9809\n",
      "Epoch 24/50\n",
      "2764/2765 [============================>.] - ETA: 0s - loss: 0.2168 - accuracy: 0.9752\n",
      "Epoch 00024: val_accuracy did not improve from 0.98178\n",
      "2765/2765 [==============================] - 13s 5ms/step - loss: 0.2167 - accuracy: 0.9752 - val_loss: 0.0695 - val_accuracy: 0.9786\n",
      "Epoch 25/50\n",
      "2757/2765 [============================>.] - ETA: 0s - loss: 0.2165 - accuracy: 0.9756\n",
      "Epoch 00025: val_accuracy did not improve from 0.98178\n",
      "2765/2765 [==============================] - 13s 5ms/step - loss: 0.2163 - accuracy: 0.9756 - val_loss: 0.0683 - val_accuracy: 0.9808\n",
      "Epoch 26/50\n",
      "2761/2765 [============================>.] - ETA: 0s - loss: 0.2181 - accuracy: 0.9755\n",
      "Epoch 00026: val_accuracy did not improve from 0.98178\n",
      "2765/2765 [==============================] - 13s 5ms/step - loss: 0.2180 - accuracy: 0.9755 - val_loss: 0.0721 - val_accuracy: 0.9780\n",
      "Epoch 27/50\n",
      "2753/2765 [============================>.] - ETA: 0s - loss: 0.2123 - accuracy: 0.9761\n",
      "Epoch 00027: val_accuracy did not improve from 0.98178\n",
      "2765/2765 [==============================] - 13s 5ms/step - loss: 0.2124 - accuracy: 0.9761 - val_loss: 0.0716 - val_accuracy: 0.9769\n",
      "Epoch 28/50\n",
      "2757/2765 [============================>.] - ETA: 0s - loss: 0.2066 - accuracy: 0.9764\n",
      "Epoch 00028: val_accuracy did not improve from 0.98178\n",
      "2765/2765 [==============================] - 13s 5ms/step - loss: 0.2067 - accuracy: 0.9763 - val_loss: 0.0666 - val_accuracy: 0.9808\n",
      "Epoch 29/50\n",
      "2755/2765 [============================>.] - ETA: 0s - loss: 0.2066 - accuracy: 0.9769\n",
      "Epoch 00029: val_accuracy did not improve from 0.98178\n",
      "2765/2765 [==============================] - 13s 5ms/step - loss: 0.2067 - accuracy: 0.9769 - val_loss: 0.0663 - val_accuracy: 0.9809\n",
      "Epoch 30/50\n",
      "2757/2765 [============================>.] - ETA: 0s - loss: 0.1978 - accuracy: 0.9776\n",
      "Epoch 00030: val_accuracy did not improve from 0.98178\n",
      "2765/2765 [==============================] - 13s 5ms/step - loss: 0.1978 - accuracy: 0.9776 - val_loss: 0.0791 - val_accuracy: 0.9744\n",
      "Epoch 31/50\n",
      "2761/2765 [============================>.] - ETA: 0s - loss: 0.2073 - accuracy: 0.9764\n",
      "Epoch 00031: val_accuracy did not improve from 0.98178\n",
      "2765/2765 [==============================] - 13s 5ms/step - loss: 0.2074 - accuracy: 0.9764 - val_loss: 0.0664 - val_accuracy: 0.9801\n",
      "Epoch 32/50\n",
      "2754/2765 [============================>.] - ETA: 0s - loss: 0.2019 - accuracy: 0.9770\n",
      "Epoch 00032: val_accuracy improved from 0.98178 to 0.98343, saving model to Trained_Model.hdf5\n",
      "2765/2765 [==============================] - 13s 5ms/step - loss: 0.2017 - accuracy: 0.9770 - val_loss: 0.0589 - val_accuracy: 0.9834\n",
      "Epoch 33/50\n",
      "2754/2765 [============================>.] - ETA: 0s - loss: 0.1986 - accuracy: 0.9775\n",
      "Epoch 00033: val_accuracy did not improve from 0.98343\n",
      "2765/2765 [==============================] - 13s 5ms/step - loss: 0.1986 - accuracy: 0.9776 - val_loss: 0.0672 - val_accuracy: 0.9793\n",
      "Epoch 34/50\n",
      "2765/2765 [==============================] - ETA: 0s - loss: 0.1947 - accuracy: 0.9777\n",
      "Epoch 00034: val_accuracy did not improve from 0.98343\n",
      "2765/2765 [==============================] - 13s 5ms/step - loss: 0.1947 - accuracy: 0.9777 - val_loss: 0.0649 - val_accuracy: 0.9817\n",
      "Epoch 35/50\n",
      "2760/2765 [============================>.] - ETA: 0s - loss: 0.1965 - accuracy: 0.9773\n",
      "Epoch 00035: val_accuracy did not improve from 0.98343\n",
      "2765/2765 [==============================] - 13s 5ms/step - loss: 0.1964 - accuracy: 0.9774 - val_loss: 0.0668 - val_accuracy: 0.9805\n",
      "Epoch 36/50\n",
      "2761/2765 [============================>.] - ETA: 0s - loss: 0.1897 - accuracy: 0.9783\n",
      "Epoch 00036: val_accuracy improved from 0.98343 to 0.98425, saving model to Trained_Model.hdf5\n",
      "2765/2765 [==============================] - 13s 5ms/step - loss: 0.1897 - accuracy: 0.9783 - val_loss: 0.0585 - val_accuracy: 0.9842\n",
      "Epoch 37/50\n",
      "2757/2765 [============================>.] - ETA: 0s - loss: 0.1959 - accuracy: 0.9778\n",
      "Epoch 00037: val_accuracy did not improve from 0.98425\n",
      "2765/2765 [==============================] - 13s 5ms/step - loss: 0.1957 - accuracy: 0.9778 - val_loss: 0.0672 - val_accuracy: 0.9809\n",
      "Epoch 38/50\n",
      "2755/2765 [============================>.] - ETA: 0s - loss: 0.1957 - accuracy: 0.9779\n",
      "Epoch 00038: val_accuracy did not improve from 0.98425\n",
      "2765/2765 [==============================] - 13s 5ms/step - loss: 0.1956 - accuracy: 0.9779 - val_loss: 0.0637 - val_accuracy: 0.9817\n",
      "Epoch 39/50\n",
      "2756/2765 [============================>.] - ETA: 0s - loss: 0.1897 - accuracy: 0.9781\n",
      "Epoch 00039: val_accuracy did not improve from 0.98425\n",
      "2765/2765 [==============================] - 13s 5ms/step - loss: 0.1894 - accuracy: 0.9781 - val_loss: 0.0716 - val_accuracy: 0.9781\n",
      "Epoch 40/50\n",
      "2759/2765 [============================>.] - ETA: 0s - loss: 0.1894 - accuracy: 0.9784\n",
      "Epoch 00040: val_accuracy did not improve from 0.98425\n",
      "2765/2765 [==============================] - 13s 5ms/step - loss: 0.1893 - accuracy: 0.9784 - val_loss: 0.0635 - val_accuracy: 0.9816\n",
      "Epoch 41/50\n",
      "2760/2765 [============================>.] - ETA: 0s - loss: 0.1887 - accuracy: 0.9784\n",
      "Epoch 00041: val_accuracy did not improve from 0.98425\n",
      "2765/2765 [==============================] - 13s 5ms/step - loss: 0.1887 - accuracy: 0.9784 - val_loss: 0.0707 - val_accuracy: 0.9776\n",
      "Epoch 42/50\n",
      "2752/2765 [============================>.] - ETA: 0s - loss: 0.1872 - accuracy: 0.9789\n",
      "Epoch 00042: val_accuracy did not improve from 0.98425\n",
      "2765/2765 [==============================] - 13s 5ms/step - loss: 0.1870 - accuracy: 0.9789 - val_loss: 0.0630 - val_accuracy: 0.9816\n",
      "Epoch 43/50\n",
      "2752/2765 [============================>.] - ETA: 0s - loss: 0.1797 - accuracy: 0.9791\n",
      "Epoch 00043: val_accuracy did not improve from 0.98425\n",
      "2765/2765 [==============================] - 12s 5ms/step - loss: 0.1795 - accuracy: 0.9791 - val_loss: 0.0594 - val_accuracy: 0.9837\n",
      "Epoch 44/50\n",
      "2758/2765 [============================>.] - ETA: 0s - loss: 0.1801 - accuracy: 0.9785\n",
      "Epoch 00044: val_accuracy did not improve from 0.98425\n",
      "2765/2765 [==============================] - 13s 5ms/step - loss: 0.1799 - accuracy: 0.9785 - val_loss: 0.0660 - val_accuracy: 0.9798\n",
      "Epoch 45/50\n",
      "2754/2765 [============================>.] - ETA: 0s - loss: 0.1850 - accuracy: 0.9788\n",
      "Epoch 00045: val_accuracy did not improve from 0.98425\n",
      "2765/2765 [==============================] - 13s 5ms/step - loss: 0.1847 - accuracy: 0.9788 - val_loss: 0.0608 - val_accuracy: 0.9823\n",
      "Epoch 46/50\n",
      "2758/2765 [============================>.] - ETA: 0s - loss: 0.1774 - accuracy: 0.9794\n",
      "Epoch 00046: val_accuracy did not improve from 0.98425\n",
      "2765/2765 [==============================] - 13s 5ms/step - loss: 0.1774 - accuracy: 0.9794 - val_loss: 0.0594 - val_accuracy: 0.9840\n",
      "Epoch 47/50\n",
      "2755/2765 [============================>.] - ETA: 0s - loss: 0.1823 - accuracy: 0.9788\n",
      "Epoch 00047: val_accuracy did not improve from 0.98425\n",
      "2765/2765 [==============================] - 13s 5ms/step - loss: 0.1821 - accuracy: 0.9788 - val_loss: 0.0594 - val_accuracy: 0.9833\n",
      "Epoch 48/50\n",
      "2758/2765 [============================>.] - ETA: 0s - loss: 0.1723 - accuracy: 0.9798\n",
      "Epoch 00048: val_accuracy improved from 0.98425 to 0.98503, saving model to Trained_Model.hdf5\n",
      "2765/2765 [==============================] - 14s 5ms/step - loss: 0.1722 - accuracy: 0.9798 - val_loss: 0.0553 - val_accuracy: 0.9850\n",
      "Epoch 49/50\n",
      "2753/2765 [============================>.] - ETA: 0s - loss: 0.1842 - accuracy: 0.9788\n",
      "Epoch 00049: val_accuracy did not improve from 0.98503\n",
      "2765/2765 [==============================] - 13s 5ms/step - loss: 0.1842 - accuracy: 0.9788 - val_loss: 0.0793 - val_accuracy: 0.9741\n",
      "Epoch 50/50\n",
      "2756/2765 [============================>.] - ETA: 0s - loss: 0.1745 - accuracy: 0.9794\n",
      "Epoch 00050: val_accuracy improved from 0.98503 to 0.98551, saving model to Trained_Model.hdf5\n",
      "2765/2765 [==============================] - 13s 5ms/step - loss: 0.1744 - accuracy: 0.9794 - val_loss: 0.0555 - val_accuracy: 0.9855\n"
     ]
    }
   ],
   "source": [
    "model=large_model()\n",
    "H=model.fit(\n",
    "    trainX,\n",
    "    trainY,\n",
    "    batch_size=128,\n",
    "    validation_data=(testX,testY),\n",
    "    steps_per_epoch=len(trainX)//128,\n",
    "    epochs=50,\n",
    "    class_weight=classWeight,\n",
    "    verbose=1,\n",
    "    callbacks=[model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7SIUOGJjdX0n"
   },
   "outputs": [],
   "source": [
    "labelNames=\"0123456789\"\n",
    "labelNames+=\"ABCDEFGHIJKLMNOPQURSUVWXYZ\"\n",
    "labelNames=[l for l in labelNames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4QnuvIavn-YQ",
    "outputId": "7e4a9124-1097-40bd-abe0-86193e64796d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labelNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 748
    },
    "colab_type": "code",
    "id": "Vj0l02b8nVmS",
    "outputId": "c2b6198e-f920-4013-d759-11fe1d3407eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.83      0.86      1381\n",
      "           1       0.99      0.99      0.99      1575\n",
      "           2       0.97      0.98      0.98      1398\n",
      "           3       0.99      0.99      0.99      1428\n",
      "           4       0.99      0.98      0.98      1365\n",
      "           5       0.95      0.96      0.95      1263\n",
      "           6       0.99      0.99      0.99      1375\n",
      "           7       0.98      0.99      0.99      1459\n",
      "           8       0.98      0.99      0.98      1365\n",
      "           9       0.99      0.98      0.99      1392\n",
      "           A       0.99      1.00      0.99      2774\n",
      "           B       0.99      0.99      0.99      1734\n",
      "           C       0.99      0.99      0.99      4682\n",
      "           D       0.95      0.99      0.97      2027\n",
      "           E       0.99      1.00      0.99      2288\n",
      "           F       0.99      0.99      0.99       232\n",
      "           G       0.97      0.99      0.98      1152\n",
      "           H       0.98      0.99      0.99      1444\n",
      "           I       0.99      0.99      0.99       224\n",
      "           J       0.98      0.98      0.98      1699\n",
      "           K       0.98      0.99      0.98      1121\n",
      "           L       0.99      0.99      0.99      2317\n",
      "           M       0.99      0.99      0.99      2467\n",
      "           N       1.00      0.99      0.99      3802\n",
      "           O       0.98      0.98      0.98     11565\n",
      "           P       1.00      0.99      0.99      3868\n",
      "           Q       0.98      0.99      0.99      1162\n",
      "           U       0.99      0.99      0.99      2313\n",
      "           R       0.99      0.99      0.99      9684\n",
      "           S       1.00      1.00      1.00      4499\n",
      "           U       0.99      0.99      0.99      5801\n",
      "           V       0.99      1.00      0.99       836\n",
      "           W       0.98      1.00      0.99      2157\n",
      "           X       0.99      1.00      0.99      1254\n",
      "           Y       0.99      0.99      0.99      2172\n",
      "           Z       0.98      0.99      0.98      1215\n",
      "\n",
      "    accuracy                           0.99     88490\n",
      "   macro avg       0.98      0.98      0.98     88490\n",
      "weighted avg       0.99      0.99      0.99     88490\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions=model.predict(testX,batch_size=128)\n",
    "print(classification_report(testY.argmax(axis=1),\n",
    "                            predictions.argmax(axis=1),target_names=labelNames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HT7OOF66oAt2"
   },
   "outputs": [],
   "source": [
    "N=np.arange(0,50)\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(N, H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "#plt.savefig(args[\"plot\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3yhcgDUYpOuv"
   },
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(H.history['accuracy'])\n",
    "plt.plot(H.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(H.history['loss'])\n",
    "plt.plot(H.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "helpers.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
